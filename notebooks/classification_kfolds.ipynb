{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a116e01d-c96b-4fd2-9447-4a5857f798c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch.utils.data import ConcatDataset, SubsetRandomSampler, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#### PARAMETERS ####\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "seed = 0\n",
    "lr = 0.001 #0.001\n",
    "momentum = 0.9\n",
    "weight_decay=3e-4\n",
    "k_folds = 5\n",
    "grad_clip=5.\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.RandomHorizontalFlip(0.3),\n",
    "            transforms.RandomRotation(degrees=(-45, 45)),\n",
    "            transforms.RandomPerspective(distortion_scale=0.4, p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                torch.nn.ModuleList(\n",
    "                    [\n",
    "                        transforms.ColorJitter(\n",
    "                            brightness=0.3, contrast=0.3, saturation=0.1, hue=0.4\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "                p=0.2,\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.RandomHorizontalFlip(0.3),\n",
    "            transforms.RandomRotation(degrees=(-45, 45)),\n",
    "            transforms.RandomPerspective(distortion_scale=0.4, p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                torch.nn.ModuleList(\n",
    "                    [\n",
    "                        transforms.ColorJitter(\n",
    "                            brightness=0.3, contrast=0.3, saturation=0.1, hue=0.4\n",
    "                        ),\n",
    "                        # transforms.RandomCrop(size=(32, 32)),\n",
    "                        # # transforms.RandomVerticalFlip(p=0.5),\n",
    "                        # transforms.Grayscale(num_output_channels=3),\n",
    "                    ]\n",
    "                ),\n",
    "                p=0.2,\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"test\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "def reset_weights(model):\n",
    "    \"\"\"\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    \"\"\"\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, \"reset_parameters\"):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        self.model = models.efficientnet_b7(pretrained=True)\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.num_features = self.model.classifier[1].in_features\n",
    "\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(self.num_features, 20),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "train_dataset = datasets.ImageFolder(\n",
    "    \"../299_cropped_bird_dataset/train_images\", transform=data_transforms[\"train\"]\n",
    ")\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    \"../299_cropped_bird_dataset/val_images\", transform=data_transforms[\"val\"]\n",
    ")\n",
    "\n",
    "dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "\n",
    "def train_epoch(model, device, dataloader, loss_fn, optimizer, lr_scheduler):\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        #lr_scheduler.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        m = nn.Softmax(dim=1)\n",
    "        probs = m(output)\n",
    "        preds_classes = probs.max(1, keepdim=True)[1]\n",
    "        train_correct += preds_classes.eq(labels.data.view_as(preds_classes)).sum()\n",
    "    \n",
    "    return train_loss, train_correct\n",
    "\n",
    "\n",
    "def valid_epoch(model, device, dataloader, loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        valid_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        m = nn.Softmax(dim=1)\n",
    "        probs = m(output)\n",
    "        preds_classes = probs.max(1, keepdim=True)[1]\n",
    "        val_correct += preds_classes.eq(labels.data.view_as(preds_classes)).sum()\n",
    "\n",
    "    return valid_loss, val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d43bfa-58cc-4a23-97c6-411c25a78ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################\n",
      "Start for model  EfficientNet\n",
      "##############################################\n",
      "\n",
      "\n",
      "##############################################\n",
      "Fold 1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EfficientNet\"\n",
    "\n",
    "print(\"##############################################\")\n",
    "print(\"Start for model \", model_name)\n",
    "print(\"##############################################\")\n",
    "print(\"\\n\")\n",
    "\n",
    "dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "foldperf = {}\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()    \n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)))):\n",
    "    print(\"##############################################\")\n",
    "    print(\"Fold {}\".format(fold + 1))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    \n",
    "    # EfficientNet\n",
    "    model = EfficientNet()\n",
    "    reset_weights(model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0, verbose=True)\n",
    "\n",
    "\n",
    "    history = {\"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_correct = train_epoch(\n",
    "            model, device, train_loader, criterion, optimizer, lr_scheduler\n",
    "        )\n",
    "        test_loss, test_correct = valid_epoch(model, device, test_loader, criterion)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print(\n",
    "            \"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(\n",
    "                epoch + 1, epochs, train_loss, test_loss, train_acc, test_acc\n",
    "            )\n",
    "        )\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        if test_acc >= 92:\n",
    "            torch.save(model, f\"../experiment/{model_name}_fold_{fold}_epoch_{epoch}.pt\")\n",
    "            print(f\"save model at ../experiment/{model_name}_fold_{fold}_epoch_{epoch}.pt\")\n",
    "\n",
    "    foldperf[\"fold{}\".format(fold + 1)] = history\n",
    "\n",
    "    torch.save(model, f\"../experiment/{model_name}_fold_{fold}.pt\")\n",
    "    print(f\"save model at ../experiment/{model_name}_fold_{fold}.pt\")\n",
    "\n",
    "testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n",
    "k=k_folds\n",
    "\n",
    "for fold, inner_dict in foldperf.items():\n",
    "    for key, value in inner_dict.items():\n",
    "        if key == \"train_acc\" or key == \"test_acc\":\n",
    "            new_list = [x.cpu() for x in inner_dict[key]]\n",
    "            inner_dict[key] = new_list\n",
    "\n",
    "for f in range(1,k+1):\n",
    "    tl_f.append(np.mean(foldperf['fold{}'.format(f)]['train_loss']))\n",
    "    testl_f.append(np.mean(foldperf['fold{}'.format(f)]['test_loss']))\n",
    "\n",
    "    ta_f.append(np.mean(foldperf['fold{}'.format(f)]['train_acc']))\n",
    "    testa_f.append(np.mean(foldperf['fold{}'.format(f)]['test_acc']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\".format(np.mean(tl_f),np.mean(testl_f),np.mean(ta_f),np.mean(testa_f))) \n",
    "\n",
    "diz_ep = {'train_loss_ep':[],'test_loss_ep':[],'train_acc_ep':[],'test_acc_ep':[]}\n",
    "\n",
    "for i in range(epochs):\n",
    "    diz_ep['train_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['train_loss'][i] for f in range(k)]))\n",
    "    diz_ep['test_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['test_loss'][i] for f in range(k)]))\n",
    "    diz_ep['train_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['train_acc'][i] for f in range(k)]))\n",
    "    diz_ep['test_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['test_acc'][i] for f in range(k)]))\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_loss_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_loss_ep'], label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.title('CNN loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracies\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_acc_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_acc_ep'], label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.title('CNN accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41427c-56bc-4aa7-b575-83f5adf5437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.res = models.resnet152(pretrained=True)\n",
    "\n",
    "        for param in self.res.conv1.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.res.bn1.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.res.layer1.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.res.layer2.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.res.layer3.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.res.avgpool = nn.AvgPool2d(10)\n",
    "        num_features2 = self.res.fc.in_features\n",
    "        self.res.fc = nn.Linear(num_features2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.res(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model_name = \"ResNet\"\n",
    "\n",
    "print(\"##############################################\")\n",
    "print(\"Start for model \", model_name)\n",
    "print(\"##############################################\")\n",
    "print(\"\\n\")\n",
    "\n",
    "dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "foldperf = {}\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()    \n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)))):\n",
    "    print(\"##############################################\")\n",
    "    print(\"Fold {}\".format(fold + 1))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    \n",
    "    # ResNet\n",
    "    model = ResNet()\n",
    "    reset_weights(model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0, verbose=True)\n",
    "\n",
    "\n",
    "    history = {\"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_correct = train_epoch(\n",
    "            model, device, train_loader, criterion, optimizer, lr_scheduler\n",
    "        )\n",
    "        test_loss, test_correct = valid_epoch(model, device, test_loader, criterion)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print(\n",
    "            \"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(\n",
    "                epoch + 1, epochs, train_loss, test_loss, train_acc, test_acc\n",
    "            )\n",
    "        )\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        if test_acc >= 92:\n",
    "            torch.save(model, f\"../experiment/{model_name}_fold_{fold}_epoch_{epoch}.pt\")\n",
    "            print(f\"save model at ../experiment/{model_name}_fold_{fold}_epoch_{epoch}.pt\")\n",
    "\n",
    "    foldperf[\"fold{}\".format(fold + 1)] = history\n",
    "\n",
    "    torch.save(model, f\"../experiment/{model_name}_fold_{fold}.pt\")\n",
    "    print(f\"save model at ../experiment/{model_name}_fold_{fold}.pt\")\n",
    "\n",
    "testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n",
    "k=k_folds\n",
    "\n",
    "for fold, inner_dict in foldperf.items():\n",
    "    for key, value in inner_dict.items():\n",
    "        if key == \"train_acc\" or key == \"test_acc\":\n",
    "            new_list = [x.cpu() for x in inner_dict[key]]\n",
    "            inner_dict[key] = new_list\n",
    "\n",
    "for f in range(1,k+1):\n",
    "    tl_f.append(np.mean(foldperf['fold{}'.format(f)]['train_loss']))\n",
    "    testl_f.append(np.mean(foldperf['fold{}'.format(f)]['test_loss']))\n",
    "\n",
    "    ta_f.append(np.mean(foldperf['fold{}'.format(f)]['train_acc']))\n",
    "    testa_f.append(np.mean(foldperf['fold{}'.format(f)]['test_acc']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\".format(np.mean(tl_f),np.mean(testl_f),np.mean(ta_f),np.mean(testa_f))) \n",
    "\n",
    "diz_ep = {'train_loss_ep':[],'test_loss_ep':[],'train_acc_ep':[],'test_acc_ep':[]}\n",
    "\n",
    "for i in range(epochs):\n",
    "    diz_ep['train_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['train_loss'][i] for f in range(k)]))\n",
    "    diz_ep['test_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['test_loss'][i] for f in range(k)]))\n",
    "    diz_ep['train_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['train_acc'][i] for f in range(k)]))\n",
    "    diz_ep['test_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['test_acc'][i] for f in range(k)]))\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_loss_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_loss_ep'], label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.title('CNN loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracies\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_acc_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_acc_ep'], label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.title('CNN accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b43a6c1-d650-4b14-81cb-77e85e935b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inceptionv3(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(Inceptionv3, self).__init__()\n",
    "\n",
    "        self.inc = models.inception_v3(pretrained=True)\n",
    "\n",
    "        for param in self.inc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.inc.aux_logits = False\n",
    "        num_features = self.inc.fc.in_features\n",
    "        self.inc.fc = nn.Linear(num_features, 1024)\n",
    "        lin3 = nn.Linear(1024, num_classes)\n",
    "        self.fc = lin3\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.inc(input)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    \n",
    "model_name = \"Inceptionv3\"\n",
    "\n",
    "print(\"##############################################\")\n",
    "print(\"Start for model \", model_name)\n",
    "print(\"##############################################\")\n",
    "print(\"\\n\")\n",
    "\n",
    "dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "foldperf = {}\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()    \n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)))):\n",
    "    print(\"##############################################\")\n",
    "    print(\"Fold {}\".format(fold + 1))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    \n",
    "    # Inceptionv3\n",
    "    model = Inceptionv3()\n",
    "    reset_weights(model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0, verbose=True)\n",
    "\n",
    "\n",
    "    history = {\"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_correct = train_epoch(\n",
    "            model, device, train_loader, criterion, optimizer, lr_scheduler\n",
    "        )\n",
    "        test_loss, test_correct = valid_epoch(model, device, test_loader, criterion)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print(\n",
    "            \"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(\n",
    "                epoch + 1, epochs, train_loss, test_loss, train_acc, test_acc\n",
    "            )\n",
    "        )\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        if test_acc >= 92:\n",
    "            torch.save(model, f\"../experiment/{model_name}_fold_{fold}_epoch_{epoch}.pt\")\n",
    "            print(f\"save model at ../experiment/{model_name}_fold_{fold}_epoch_{epoch}.pt\")\n",
    "\n",
    "    foldperf[\"fold{}\".format(fold + 1)] = history\n",
    "\n",
    "    torch.save(model, f\"../experiment/{model_name}_fold_{fold}.pt\")\n",
    "    print(f\"save model at ../experiment/{model_name}_fold_{fold}.pt\")\n",
    "\n",
    "testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n",
    "k=k_folds\n",
    "\n",
    "for fold, inner_dict in foldperf.items():\n",
    "    for key, value in inner_dict.items():\n",
    "        if key == \"train_acc\" or key == \"test_acc\":\n",
    "            new_list = [x.cpu() for x in inner_dict[key]]\n",
    "            inner_dict[key] = new_list\n",
    "\n",
    "for f in range(1,k+1):\n",
    "    tl_f.append(np.mean(foldperf['fold{}'.format(f)]['train_loss']))\n",
    "    testl_f.append(np.mean(foldperf['fold{}'.format(f)]['test_loss']))\n",
    "\n",
    "    ta_f.append(np.mean(foldperf['fold{}'.format(f)]['train_acc']))\n",
    "    testa_f.append(np.mean(foldperf['fold{}'.format(f)]['test_acc']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\".format(np.mean(tl_f),np.mean(testl_f),np.mean(ta_f),np.mean(testa_f))) \n",
    "\n",
    "diz_ep = {'train_loss_ep':[],'test_loss_ep':[],'train_acc_ep':[],'test_acc_ep':[]}\n",
    "\n",
    "for i in range(epochs):\n",
    "    diz_ep['train_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['train_loss'][i] for f in range(k)]))\n",
    "    diz_ep['test_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['test_loss'][i] for f in range(k)]))\n",
    "    diz_ep['train_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['train_acc'][i] for f in range(k)]))\n",
    "    diz_ep['test_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['test_acc'][i] for f in range(k)]))\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_loss_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_loss_ep'], label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.title('CNN loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracies\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_acc_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_acc_ep'], label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.title('CNN accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1b749-8b3a-43a9-a209-71d5d3e66e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(\n",
    "        \"../299_cropped_bird_dataset/test_images\", transform=data_transforms[\"test\"]\n",
    "    ),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "preds = np.array([])\n",
    "\n",
    "## Choose the model\n",
    "state_dict = torch.load(model_path)\n",
    "model = ResNet()\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (data, labels) in tqdm(enumerate(test_loader, 0)):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        output1 = model(data)\n",
    "        sm = nn.Softmax(dim=1)(output1)\n",
    "        pred = sm.max(1, keepdim=True)[1]    \n",
    "        preds = np.hstack((preds, torch.squeeze(pred).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6911da9-e5e0-4b1f-a999-aa8080f8c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../299_cropped_submission.csv\", \"w\")\n",
    "f.write(\"Id,Category\\n\")\n",
    "for (n,_),p in zip(test_loader.dataset.samples,preds):\n",
    "    f.write(\"{},{}\\n\".format(n.split('/')[-1].split('.')[0], int(p)))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
