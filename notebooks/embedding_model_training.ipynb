{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37246bcb-1f11-43e4-af11-f04500ebee20",
   "metadata": {},
   "source": [
    "## Start model training:\n",
    "\n",
    "- Simple classifier that uses cropped images from detectron2\n",
    "- Even the images that are not cropped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166a5650-bc9f-4779-a154-e88f9ac9ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim \n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import argparse\n",
    "import os\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36454a5-69a3-4c11-bd3b-9ce294ec240e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa19032ff00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dir = '../embeddings'\n",
    "data = \"../cropped_bird_dataset\"\n",
    "#img_size = 299\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 3e-4\n",
    "grad_clip = 5.\n",
    "seed = 1\n",
    "use_cuda = False\n",
    "experiment='../experiment'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6fb96d7-2580-4f0d-b7ab-8f83651c2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layers network of the features\n",
    "\n",
    "# Features and labels\n",
    "features_train = torch.load(os.path.join(load_dir, \"birds_features_train.pt\"), map_location=torch.device(device))\n",
    "labels_train = torch.load(os.path.join(load_dir, \"birds_labels_train.pt\"), map_location=torch.device(device))\n",
    "\n",
    "features_val = torch.load(os.path.join(load_dir, \"birds_features_val.pt\"), map_location=torch.device(device))\n",
    "labels_val = torch.load(os.path.join(load_dir, \"birds_labels_val.pt\"), map_location=torch.device(device))\n",
    "\n",
    "# X = features.to(\"cpu\").numpy()\n",
    "\n",
    "# Dataloaders\n",
    "features_tensor = torch.stack([i for i in features_train])\n",
    "labels_tensor = torch.stack([i for i in labels_train])\n",
    "train_data = torch.utils.data.TensorDataset(features_tensor, labels_tensor) \n",
    "\n",
    "features_tensor = torch.stack([torch.Tensor(i) for i in features_val])\n",
    "labels_tensor = torch.stack([i for i in labels_val])\n",
    "val_data = torch.utils.data.TensorDataset(features_tensor,labels_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f231d62-41de-42e9-9cf6-6ea158b3115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade loss\n",
    "\n",
    "class HardBatchMiningTripletLoss(torch.nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining of samples in a batch.\n",
    "\n",
    "    Reference:\n",
    "        Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
    "    Args:\n",
    "        margin (float, optional): margin for triplet. Default is 0.3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=0.3):\n",
    "        super(HardBatchMiningTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.ranking_loss = torch.nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (torch.Tensor): feature matrix with shape (batch_size, feat_dim).\n",
    "            targets (torch.LongTensor): ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        n = inputs.size(0)\n",
    "\n",
    "        # TASK: Compute the pairwise euclidean distance between all n feature vectors.\n",
    "        # Hint: We recommend computing the actual euclidean distance (not squared).\n",
    "        # For numerical stability, you can do sth. like:\n",
    "        # distance_matrix = distance_matrix.clamp(min=1e-12).sqrt()\n",
    "        input1 = inputs\n",
    "        input2 = inputs.transpose(0, 1)\n",
    "        matrix_product = torch.matmul(input1, input2)\n",
    "        diag = torch.diag(matrix_product)\n",
    "        distance_matrix = diag.unsqueeze(0) - 2.0 * matrix_product + diag.unsqueeze(1)\n",
    "        distance_matrix = distance_matrix.clamp(min=1e-12).sqrt()\n",
    "\n",
    "        # TASK: For each sample (image), find the hardest positive and hardest negative sample.\n",
    "        # The targets are a vector that encode the class label for each of the n samples.\n",
    "        # Pairs of samples with the SAME class can form a positive sample.\n",
    "        # Pairs of samples with a DIFFERENT class can form a negative sample.\n",
    "        #\n",
    "        # For this task, you will need to loop over all samples, and for each one\n",
    "        # find the hardest positive sample and the hardest negative sample.\n",
    "        # The distances are then added to the following lists.\n",
    "        # Please think about what hardest means for positive and negative pairs.\n",
    "        # Reminder: Positive pairs should be as close as possible, while\n",
    "        # negative pairs should be quite far apart.\n",
    "\n",
    "        distance_positive_pairs, distance_negative_pairs = [], []\n",
    "        print(distance_matrix.shape)\n",
    "        for i in range(n):\n",
    "            current_label = targets[i].item()\n",
    "            mask = targets.eq(current_label)\n",
    "            distance_positive = torch.max(torch.masked_select(distance_matrix[i, :], mask))\n",
    "            print(\"pos\", distance_positive.shape)\n",
    "            distance_negative = torch.min(torch.masked_select(distance_matrix[i, :], torch.logical_not(mask)))\n",
    "            print(\"neg\", distance_negative.shape)\n",
    "            distance_positive_pairs.append(distance_positive)\n",
    "            distance_negative_pairs.append(distance_negative)\n",
    "\n",
    "        # TASK: Convert the created lists into 1D pytorch tensors. Please never\n",
    "        # convert the tensors to numpy or raw python format, as you want to backpropagate\n",
    "        # the loss, i.e., the above lists should only contain pytorch tensors.\n",
    "        # Hint: Checkout the pytorch documentation.\n",
    "        distance_positive_pairs = torch.tensor(distance_positive_pairs, device=device)\n",
    "        distance_negative_pairs = torch.tensor(distance_negative_pairs, device=device)\n",
    "\n",
    "        # The ranking loss will compute the triplet loss with the margin.\n",
    "        # loss = max(0, -1*(neg_dist - pos_dist) + margin)\n",
    "        # This is done already, no need to change anything.\n",
    "        y = torch.ones_like(distance_negative_pairs)\n",
    "        return self.ranking_loss(distance_negative_pairs, distance_positive_pairs, y)\n",
    "\n",
    "\n",
    "class CombinedLoss(object):\n",
    "    def __init__(self, margin=0.3, weight_triplet=1.0, weight_ce=1.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.triplet_loss = HardBatchMiningTripletLoss()  # <--- Your code is used here!\n",
    "        self.cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "        self.weight_triplet = weight_triplet\n",
    "        self.weight_ce = weight_ce\n",
    "\n",
    "    def __call__(self, logits, features):\n",
    "        loss = 0.0\n",
    "        loss_summary = {}\n",
    "        if self.weight_triplet > 0.0:\n",
    "            loss_t = self.triplet_loss(features) * self.weight_triplet\n",
    "            loss += loss_t\n",
    "            loss_summary['Triplet Loss'] = loss_t\n",
    "\n",
    "        if self.weight_ce > 0.0:\n",
    "            loss_ce = self.cross_entropy(logits) * self.weight_ce\n",
    "            loss += loss_ce\n",
    "            loss_summary['CE Loss'] = loss_ce\n",
    "\n",
    "        loss_summary['Loss'] = loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ba24ef-92f4-4c28-a276-29434eee13bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Classifier(features_train[0].shape[0])\n",
    "\n",
    "if use_cuda:\n",
    "    print('Using GPU')\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "# Optimizer, LR, and criterion\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "# Training functions\n",
    "def train_classifier(model, train_loader, optimizer, lr_scheduler, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda().long())\n",
    "                \n",
    "        else:\n",
    "              data, target = Variable(data), Variable(target.long())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #target = target.squeeze(1)\n",
    "        #loss = triplet_loss.forward(output, target)\n",
    "        #loss.requires_grad = True\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "def validation_classifier(model, criterion, val_loader):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            if use_cuda:\n",
    "                data, target = Variable(data.cuda()), Variable(target.cuda().long())\n",
    "            else:\n",
    "                data, target = Variable(data), Variable(target.long())\n",
    "            output = model(data)\n",
    "            \n",
    "            #validation_loss += triplet_loss.forward(output, target).data.item()\n",
    "        \n",
    "            #target = target.squeeze(1)\n",
    "            # sum up batch loss\n",
    "            validation_loss += criterion(output, target).data.item()\n",
    "            \n",
    "            # get the index of the max log-probability\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        validation_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    return(100. * correct / len(val_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "204b891a-9e47-44c6-8676-63bf8b687bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1082 (0%)]\tLoss: 2.981313\n",
      "Train Epoch: 1 [320/1082 (29%)]\tLoss: 3.008550\n",
      "Train Epoch: 1 [640/1082 (59%)]\tLoss: 3.020131\n",
      "Train Epoch: 1 [960/1082 (88%)]\tLoss: 3.026696\n",
      "\n",
      "Validation set: Average loss: 0.1166, Accuracy: 17/103 (17%)\n",
      "Train Epoch: 2 [0/1082 (0%)]\tLoss: 2.969938\n",
      "Train Epoch: 2 [320/1082 (29%)]\tLoss: 2.984365\n",
      "Train Epoch: 2 [640/1082 (59%)]\tLoss: 2.964207\n",
      "Train Epoch: 2 [960/1082 (88%)]\tLoss: 2.968841\n",
      "\n",
      "Validation set: Average loss: 0.1162, Accuracy: 19/103 (18%)\n",
      "Train Epoch: 3 [0/1082 (0%)]\tLoss: 2.971701\n",
      "Train Epoch: 3 [320/1082 (29%)]\tLoss: 2.957095\n",
      "Train Epoch: 3 [640/1082 (59%)]\tLoss: 2.986103\n",
      "Train Epoch: 3 [960/1082 (88%)]\tLoss: 2.988990\n",
      "\n",
      "Validation set: Average loss: 0.1142, Accuracy: 25/103 (24%)\n",
      "Train Epoch: 4 [0/1082 (0%)]\tLoss: 2.932263\n",
      "Train Epoch: 4 [320/1082 (29%)]\tLoss: 2.990494\n",
      "Train Epoch: 4 [640/1082 (59%)]\tLoss: 2.919428\n",
      "Train Epoch: 4 [960/1082 (88%)]\tLoss: 2.970253\n",
      "\n",
      "Validation set: Average loss: 0.1122, Accuracy: 19/103 (18%)\n",
      "Train Epoch: 5 [0/1082 (0%)]\tLoss: 2.931136\n",
      "Train Epoch: 5 [320/1082 (29%)]\tLoss: 2.924986\n",
      "Train Epoch: 5 [640/1082 (59%)]\tLoss: 2.904731\n",
      "Train Epoch: 5 [960/1082 (88%)]\tLoss: 2.921874\n",
      "\n",
      "Validation set: Average loss: 0.1119, Accuracy: 28/103 (27%)\n",
      "Train Epoch: 6 [0/1082 (0%)]\tLoss: 2.885245\n",
      "Train Epoch: 6 [320/1082 (29%)]\tLoss: 2.892089\n",
      "Train Epoch: 6 [640/1082 (59%)]\tLoss: 2.950821\n",
      "Train Epoch: 6 [960/1082 (88%)]\tLoss: 2.872232\n",
      "\n",
      "Validation set: Average loss: 0.1139, Accuracy: 16/103 (16%)\n",
      "Train Epoch: 7 [0/1082 (0%)]\tLoss: 2.744936\n",
      "Train Epoch: 7 [320/1082 (29%)]\tLoss: 2.803753\n",
      "Train Epoch: 7 [640/1082 (59%)]\tLoss: 2.721165\n",
      "Train Epoch: 7 [960/1082 (88%)]\tLoss: 2.737772\n",
      "\n",
      "Validation set: Average loss: 0.1039, Accuracy: 40/103 (39%)\n",
      "Train Epoch: 8 [0/1082 (0%)]\tLoss: 2.816153\n",
      "Train Epoch: 8 [320/1082 (29%)]\tLoss: 2.844112\n",
      "Train Epoch: 8 [640/1082 (59%)]\tLoss: 2.814902\n",
      "Train Epoch: 8 [960/1082 (88%)]\tLoss: 2.762955\n",
      "\n",
      "Validation set: Average loss: 0.1024, Accuracy: 38/103 (37%)\n",
      "Train Epoch: 9 [0/1082 (0%)]\tLoss: 2.860235\n",
      "Train Epoch: 9 [320/1082 (29%)]\tLoss: 2.806726\n",
      "Train Epoch: 9 [640/1082 (59%)]\tLoss: 2.823178\n",
      "Train Epoch: 9 [960/1082 (88%)]\tLoss: 2.801316\n",
      "\n",
      "Validation set: Average loss: 0.1002, Accuracy: 44/103 (43%)\n",
      "Train Epoch: 10 [0/1082 (0%)]\tLoss: 2.841028\n",
      "Train Epoch: 10 [320/1082 (29%)]\tLoss: 2.663897\n",
      "Train Epoch: 10 [640/1082 (59%)]\tLoss: 2.847514\n",
      "Train Epoch: 10 [960/1082 (88%)]\tLoss: 2.761137\n",
      "\n",
      "Validation set: Average loss: 0.0917, Accuracy: 46/103 (45%)\n",
      "Train Epoch: 11 [0/1082 (0%)]\tLoss: 2.556264\n",
      "Train Epoch: 11 [320/1082 (29%)]\tLoss: 2.692904\n",
      "Train Epoch: 11 [640/1082 (59%)]\tLoss: 2.645957\n",
      "Train Epoch: 11 [960/1082 (88%)]\tLoss: 2.486171\n",
      "\n",
      "Validation set: Average loss: 0.0890, Accuracy: 52/103 (50%)\n",
      "Train Epoch: 12 [0/1082 (0%)]\tLoss: 2.498366\n",
      "Train Epoch: 12 [320/1082 (29%)]\tLoss: 2.680238\n",
      "Train Epoch: 12 [640/1082 (59%)]\tLoss: 2.626285\n",
      "Train Epoch: 12 [960/1082 (88%)]\tLoss: 2.694385\n",
      "\n",
      "Validation set: Average loss: 0.0855, Accuracy: 44/103 (43%)\n",
      "Train Epoch: 13 [0/1082 (0%)]\tLoss: 2.509080\n",
      "Train Epoch: 13 [320/1082 (29%)]\tLoss: 2.662603\n",
      "Train Epoch: 13 [640/1082 (59%)]\tLoss: 2.551739\n",
      "Train Epoch: 13 [960/1082 (88%)]\tLoss: 2.597056\n",
      "\n",
      "Validation set: Average loss: 0.0813, Accuracy: 58/103 (56%)\n",
      "Train Epoch: 14 [0/1082 (0%)]\tLoss: 2.071704\n",
      "Train Epoch: 14 [320/1082 (29%)]\tLoss: 2.130580\n",
      "Train Epoch: 14 [640/1082 (59%)]\tLoss: 2.297730\n",
      "Train Epoch: 14 [960/1082 (88%)]\tLoss: 2.347172\n",
      "\n",
      "Validation set: Average loss: 0.0792, Accuracy: 65/103 (63%)\n",
      "Train Epoch: 15 [0/1082 (0%)]\tLoss: 2.070627\n",
      "Train Epoch: 15 [320/1082 (29%)]\tLoss: 2.177732\n",
      "Train Epoch: 15 [640/1082 (59%)]\tLoss: 2.276227\n",
      "Train Epoch: 15 [960/1082 (88%)]\tLoss: 2.461654\n",
      "\n",
      "Validation set: Average loss: 0.0825, Accuracy: 52/103 (50%)\n",
      "Train Epoch: 16 [0/1082 (0%)]\tLoss: 1.971370\n",
      "Train Epoch: 16 [320/1082 (29%)]\tLoss: 2.243338\n",
      "Train Epoch: 16 [640/1082 (59%)]\tLoss: 2.071561\n",
      "Train Epoch: 16 [960/1082 (88%)]\tLoss: 1.940336\n",
      "\n",
      "Validation set: Average loss: 0.0632, Accuracy: 69/103 (67%)\n",
      "Train Epoch: 17 [0/1082 (0%)]\tLoss: 1.775431\n",
      "Train Epoch: 17 [320/1082 (29%)]\tLoss: 1.404848\n",
      "Train Epoch: 17 [640/1082 (59%)]\tLoss: 1.920696\n",
      "Train Epoch: 17 [960/1082 (88%)]\tLoss: 1.658291\n",
      "\n",
      "Validation set: Average loss: 0.0639, Accuracy: 60/103 (58%)\n",
      "Train Epoch: 18 [0/1082 (0%)]\tLoss: 1.528696\n",
      "Train Epoch: 18 [320/1082 (29%)]\tLoss: 1.966087\n",
      "Train Epoch: 18 [640/1082 (59%)]\tLoss: 1.832347\n",
      "Train Epoch: 18 [960/1082 (88%)]\tLoss: 1.842164\n",
      "\n",
      "Validation set: Average loss: 0.0692, Accuracy: 65/103 (63%)\n",
      "Train Epoch: 19 [0/1082 (0%)]\tLoss: 1.638418\n",
      "Train Epoch: 19 [320/1082 (29%)]\tLoss: 1.218004\n",
      "Train Epoch: 19 [640/1082 (59%)]\tLoss: 1.182161\n",
      "Train Epoch: 19 [960/1082 (88%)]\tLoss: 1.411652\n",
      "\n",
      "Validation set: Average loss: 0.0582, Accuracy: 66/103 (64%)\n",
      "Train Epoch: 20 [0/1082 (0%)]\tLoss: 1.120510\n",
      "Train Epoch: 20 [320/1082 (29%)]\tLoss: 1.136176\n",
      "Train Epoch: 20 [640/1082 (59%)]\tLoss: 1.430488\n",
      "Train Epoch: 20 [960/1082 (88%)]\tLoss: 1.164766\n",
      "\n",
      "Validation set: Average loss: 0.0576, Accuracy: 70/103 (68%)\n",
      "Train Epoch: 21 [0/1082 (0%)]\tLoss: 1.296986\n",
      "Train Epoch: 21 [320/1082 (29%)]\tLoss: 1.350304\n",
      "Train Epoch: 21 [640/1082 (59%)]\tLoss: 1.206230\n",
      "Train Epoch: 21 [960/1082 (88%)]\tLoss: 1.490635\n",
      "\n",
      "Validation set: Average loss: 0.0607, Accuracy: 62/103 (60%)\n",
      "Train Epoch: 22 [0/1082 (0%)]\tLoss: 1.036251\n",
      "Train Epoch: 22 [320/1082 (29%)]\tLoss: 0.827470\n",
      "Train Epoch: 22 [640/1082 (59%)]\tLoss: 0.703007\n",
      "Train Epoch: 22 [960/1082 (88%)]\tLoss: 0.943763\n",
      "\n",
      "Validation set: Average loss: 0.0488, Accuracy: 71/103 (69%)\n",
      "Saved model to ../experiment/model_22.pth. You can run `python evaluate.py --model ../experiment/model_22.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 23 [0/1082 (0%)]\tLoss: 0.719398\n",
      "Train Epoch: 23 [320/1082 (29%)]\tLoss: 0.663289\n",
      "Train Epoch: 23 [640/1082 (59%)]\tLoss: 0.841317\n",
      "Train Epoch: 23 [960/1082 (88%)]\tLoss: 0.847411\n",
      "\n",
      "Validation set: Average loss: 0.0472, Accuracy: 71/103 (69%)\n",
      "Saved model to ../experiment/model_23.pth. You can run `python evaluate.py --model ../experiment/model_23.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 24 [0/1082 (0%)]\tLoss: 0.783119\n",
      "Train Epoch: 24 [320/1082 (29%)]\tLoss: 0.666333\n",
      "Train Epoch: 24 [640/1082 (59%)]\tLoss: 1.061062\n",
      "Train Epoch: 24 [960/1082 (88%)]\tLoss: 0.717685\n",
      "\n",
      "Validation set: Average loss: 0.0530, Accuracy: 67/103 (65%)\n",
      "Train Epoch: 25 [0/1082 (0%)]\tLoss: 0.569965\n",
      "Train Epoch: 25 [320/1082 (29%)]\tLoss: 0.553970\n",
      "Train Epoch: 25 [640/1082 (59%)]\tLoss: 0.409637\n",
      "Train Epoch: 25 [960/1082 (88%)]\tLoss: 0.428484\n",
      "\n",
      "Validation set: Average loss: 0.0421, Accuracy: 71/103 (69%)\n",
      "Saved model to ../experiment/model_25.pth. You can run `python evaluate.py --model ../experiment/model_25.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 26 [0/1082 (0%)]\tLoss: 0.304171\n",
      "Train Epoch: 26 [320/1082 (29%)]\tLoss: 0.270678\n",
      "Train Epoch: 26 [640/1082 (59%)]\tLoss: 0.329361\n",
      "Train Epoch: 26 [960/1082 (88%)]\tLoss: 0.308649\n",
      "\n",
      "Validation set: Average loss: 0.0469, Accuracy: 72/103 (70%)\n",
      "Saved model to ../experiment/model_26.pth. You can run `python evaluate.py --model ../experiment/model_26.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 27 [0/1082 (0%)]\tLoss: 0.366210\n",
      "Train Epoch: 27 [320/1082 (29%)]\tLoss: 0.533735\n",
      "Train Epoch: 27 [640/1082 (59%)]\tLoss: 0.450683\n",
      "Train Epoch: 27 [960/1082 (88%)]\tLoss: 0.390373\n",
      "\n",
      "Validation set: Average loss: 0.0543, Accuracy: 61/103 (59%)\n",
      "Train Epoch: 28 [0/1082 (0%)]\tLoss: 0.393156\n",
      "Train Epoch: 28 [320/1082 (29%)]\tLoss: 0.256742\n",
      "Train Epoch: 28 [640/1082 (59%)]\tLoss: 0.184038\n",
      "Train Epoch: 28 [960/1082 (88%)]\tLoss: 0.238399\n",
      "\n",
      "Validation set: Average loss: 0.0449, Accuracy: 71/103 (69%)\n",
      "Saved model to ../experiment/model_28.pth. You can run `python evaluate.py --model ../experiment/model_28.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 29 [0/1082 (0%)]\tLoss: 0.178613\n",
      "Train Epoch: 29 [320/1082 (29%)]\tLoss: 0.127682\n",
      "Train Epoch: 29 [640/1082 (59%)]\tLoss: 0.137230\n",
      "Train Epoch: 29 [960/1082 (88%)]\tLoss: 0.197399\n",
      "\n",
      "Validation set: Average loss: 0.0536, Accuracy: 67/103 (65%)\n",
      "Train Epoch: 30 [0/1082 (0%)]\tLoss: 0.153291\n",
      "Train Epoch: 30 [320/1082 (29%)]\tLoss: 0.193850\n",
      "Train Epoch: 30 [640/1082 (59%)]\tLoss: 0.185925\n",
      "Train Epoch: 30 [960/1082 (88%)]\tLoss: 0.244387\n",
      "\n",
      "Validation set: Average loss: 0.0493, Accuracy: 71/103 (69%)\n",
      "Saved model to ../experiment/model_30.pth. You can run `python evaluate.py --model ../experiment/model_30.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 31 [0/1082 (0%)]\tLoss: 0.114140\n",
      "Train Epoch: 31 [320/1082 (29%)]\tLoss: 0.184313\n",
      "Train Epoch: 31 [640/1082 (59%)]\tLoss: 0.094497\n",
      "Train Epoch: 31 [960/1082 (88%)]\tLoss: 0.079685\n",
      "\n",
      "Validation set: Average loss: 0.0420, Accuracy: 75/103 (73%)\n",
      "Saved model to ../experiment/model_31.pth. You can run `python evaluate.py --model ../experiment/model_31.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 32 [0/1082 (0%)]\tLoss: 0.084649\n",
      "Train Epoch: 32 [320/1082 (29%)]\tLoss: 0.076667\n",
      "Train Epoch: 32 [640/1082 (59%)]\tLoss: 0.084461\n",
      "Train Epoch: 32 [960/1082 (88%)]\tLoss: 0.060857\n",
      "\n",
      "Validation set: Average loss: 0.0384, Accuracy: 71/103 (69%)\n",
      "Saved model to ../experiment/model_32.pth. You can run `python evaluate.py --model ../experiment/model_32.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 33 [0/1082 (0%)]\tLoss: 0.061351\n",
      "Train Epoch: 33 [320/1082 (29%)]\tLoss: 0.075791\n",
      "Train Epoch: 33 [640/1082 (59%)]\tLoss: 0.062727\n",
      "Train Epoch: 33 [960/1082 (88%)]\tLoss: 0.048133\n",
      "\n",
      "Validation set: Average loss: 0.0417, Accuracy: 69/103 (67%)\n",
      "Train Epoch: 34 [0/1082 (0%)]\tLoss: 0.047734\n",
      "Train Epoch: 34 [320/1082 (29%)]\tLoss: 0.045987\n",
      "Train Epoch: 34 [640/1082 (59%)]\tLoss: 0.038131\n",
      "Train Epoch: 34 [960/1082 (88%)]\tLoss: 0.048914\n",
      "\n",
      "Validation set: Average loss: 0.0418, Accuracy: 72/103 (70%)\n",
      "Saved model to ../experiment/model_34.pth. You can run `python evaluate.py --model ../experiment/model_34.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 35 [0/1082 (0%)]\tLoss: 0.042777\n",
      "Train Epoch: 35 [320/1082 (29%)]\tLoss: 0.032308\n",
      "Train Epoch: 35 [640/1082 (59%)]\tLoss: 0.038067\n",
      "Train Epoch: 35 [960/1082 (88%)]\tLoss: 0.044689\n",
      "\n",
      "Validation set: Average loss: 0.0397, Accuracy: 74/103 (72%)\n",
      "Saved model to ../experiment/model_35.pth. You can run `python evaluate.py --model ../experiment/model_35.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 36 [0/1082 (0%)]\tLoss: 0.032966\n",
      "Train Epoch: 36 [320/1082 (29%)]\tLoss: 0.039876\n",
      "Train Epoch: 36 [640/1082 (59%)]\tLoss: 0.038457\n",
      "Train Epoch: 36 [960/1082 (88%)]\tLoss: 0.034480\n",
      "\n",
      "Validation set: Average loss: 0.0453, Accuracy: 72/103 (70%)\n",
      "Saved model to ../experiment/model_36.pth. You can run `python evaluate.py --model ../experiment/model_36.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 37 [0/1082 (0%)]\tLoss: 0.031930\n",
      "Train Epoch: 37 [320/1082 (29%)]\tLoss: 0.031904\n",
      "Train Epoch: 37 [640/1082 (59%)]\tLoss: 0.028473\n",
      "Train Epoch: 37 [960/1082 (88%)]\tLoss: 0.028703\n",
      "\n",
      "Validation set: Average loss: 0.0395, Accuracy: 76/103 (74%)\n",
      "Saved model to ../experiment/model_37.pth. You can run `python evaluate.py --model ../experiment/model_37.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 38 [0/1082 (0%)]\tLoss: 0.028596\n",
      "Train Epoch: 38 [320/1082 (29%)]\tLoss: 0.026569\n",
      "Train Epoch: 38 [640/1082 (59%)]\tLoss: 0.027543\n",
      "Train Epoch: 38 [960/1082 (88%)]\tLoss: 0.027363\n",
      "\n",
      "Validation set: Average loss: 0.0432, Accuracy: 73/103 (71%)\n",
      "Saved model to ../experiment/model_38.pth. You can run `python evaluate.py --model ../experiment/model_38.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 39 [0/1082 (0%)]\tLoss: 0.027820\n",
      "Train Epoch: 39 [320/1082 (29%)]\tLoss: 0.023982\n",
      "Train Epoch: 39 [640/1082 (59%)]\tLoss: 0.030786\n",
      "Train Epoch: 39 [960/1082 (88%)]\tLoss: 0.026107\n",
      "\n",
      "Validation set: Average loss: 0.0455, Accuracy: 74/103 (72%)\n",
      "Saved model to ../experiment/model_39.pth. You can run `python evaluate.py --model ../experiment/model_39.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 40 [0/1082 (0%)]\tLoss: 0.021225\n",
      "Train Epoch: 40 [320/1082 (29%)]\tLoss: 0.022345\n",
      "Train Epoch: 40 [640/1082 (59%)]\tLoss: 0.021535\n",
      "Train Epoch: 40 [960/1082 (88%)]\tLoss: 0.021261\n",
      "\n",
      "Validation set: Average loss: 0.0410, Accuracy: 75/103 (73%)\n",
      "Saved model to ../experiment/model_40.pth. You can run `python evaluate.py --model ../experiment/model_40.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 41 [0/1082 (0%)]\tLoss: 0.019279\n",
      "Train Epoch: 41 [320/1082 (29%)]\tLoss: 0.024989\n",
      "Train Epoch: 41 [640/1082 (59%)]\tLoss: 0.023477\n",
      "Train Epoch: 41 [960/1082 (88%)]\tLoss: 0.017691\n",
      "\n",
      "Validation set: Average loss: 0.0415, Accuracy: 73/103 (71%)\n",
      "Saved model to ../experiment/model_41.pth. You can run `python evaluate.py --model ../experiment/model_41.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 42 [0/1082 (0%)]\tLoss: 0.020064\n",
      "Train Epoch: 42 [320/1082 (29%)]\tLoss: 0.023564\n",
      "Train Epoch: 42 [640/1082 (59%)]\tLoss: 0.018745\n",
      "Train Epoch: 42 [960/1082 (88%)]\tLoss: 0.022910\n",
      "\n",
      "Validation set: Average loss: 0.0426, Accuracy: 75/103 (73%)\n",
      "Saved model to ../experiment/model_42.pth. You can run `python evaluate.py --model ../experiment/model_42.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 43 [0/1082 (0%)]\tLoss: 0.017164\n",
      "Train Epoch: 43 [320/1082 (29%)]\tLoss: 0.016863\n",
      "Train Epoch: 43 [640/1082 (59%)]\tLoss: 0.019288\n",
      "Train Epoch: 43 [960/1082 (88%)]\tLoss: 0.016275\n",
      "\n",
      "Validation set: Average loss: 0.0413, Accuracy: 75/103 (73%)\n",
      "Saved model to ../experiment/model_43.pth. You can run `python evaluate.py --model ../experiment/model_43.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 44 [0/1082 (0%)]\tLoss: 0.015586\n",
      "Train Epoch: 44 [320/1082 (29%)]\tLoss: 0.017863\n",
      "Train Epoch: 44 [640/1082 (59%)]\tLoss: 0.017432\n",
      "Train Epoch: 44 [960/1082 (88%)]\tLoss: 0.014484\n",
      "\n",
      "Validation set: Average loss: 0.0422, Accuracy: 74/103 (72%)\n",
      "Saved model to ../experiment/model_44.pth. You can run `python evaluate.py --model ../experiment/model_44.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 45 [0/1082 (0%)]\tLoss: 0.016351\n",
      "Train Epoch: 45 [320/1082 (29%)]\tLoss: 0.017460\n",
      "Train Epoch: 45 [640/1082 (59%)]\tLoss: 0.014926\n",
      "Train Epoch: 45 [960/1082 (88%)]\tLoss: 0.016922\n",
      "\n",
      "Validation set: Average loss: 0.0408, Accuracy: 74/103 (72%)\n",
      "Saved model to ../experiment/model_45.pth. You can run `python evaluate.py --model ../experiment/model_45.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 46 [0/1082 (0%)]\tLoss: 0.013903\n",
      "Train Epoch: 46 [320/1082 (29%)]\tLoss: 0.013075\n",
      "Train Epoch: 46 [640/1082 (59%)]\tLoss: 0.012913\n",
      "Train Epoch: 46 [960/1082 (88%)]\tLoss: 0.016703\n",
      "\n",
      "Validation set: Average loss: 0.0412, Accuracy: 75/103 (73%)\n",
      "Saved model to ../experiment/model_46.pth. You can run `python evaluate.py --model ../experiment/model_46.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 47 [0/1082 (0%)]\tLoss: 0.015509\n",
      "Train Epoch: 47 [320/1082 (29%)]\tLoss: 0.015459\n",
      "Train Epoch: 47 [640/1082 (59%)]\tLoss: 0.016268\n",
      "Train Epoch: 47 [960/1082 (88%)]\tLoss: 0.013479\n",
      "\n",
      "Validation set: Average loss: 0.0423, Accuracy: 74/103 (72%)\n",
      "Saved model to ../experiment/model_47.pth. You can run `python evaluate.py --model ../experiment/model_47.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 48 [0/1082 (0%)]\tLoss: 0.017283\n",
      "Train Epoch: 48 [320/1082 (29%)]\tLoss: 0.013979\n",
      "Train Epoch: 48 [640/1082 (59%)]\tLoss: 0.012849\n",
      "Train Epoch: 48 [960/1082 (88%)]\tLoss: 0.012729\n",
      "\n",
      "Validation set: Average loss: 0.0420, Accuracy: 74/103 (72%)\n",
      "Saved model to ../experiment/model_48.pth. You can run `python evaluate.py --model ../experiment/model_48.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 49 [0/1082 (0%)]\tLoss: 0.015058\n",
      "Train Epoch: 49 [320/1082 (29%)]\tLoss: 0.010970\n",
      "Train Epoch: 49 [640/1082 (59%)]\tLoss: 0.014850\n",
      "Train Epoch: 49 [960/1082 (88%)]\tLoss: 0.014084\n",
      "\n",
      "Validation set: Average loss: 0.0415, Accuracy: 75/103 (73%)\n",
      "Saved model to ../experiment/model_49.pth. You can run `python evaluate.py --model ../experiment/model_49.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 50 [0/1082 (0%)]\tLoss: 0.010560\n",
      "Train Epoch: 50 [320/1082 (29%)]\tLoss: 0.015384\n",
      "Train Epoch: 50 [640/1082 (59%)]\tLoss: 0.013364\n",
      "Train Epoch: 50 [960/1082 (88%)]\tLoss: 0.011828\n",
      "\n",
      "Validation set: Average loss: 0.0417, Accuracy: 77/103 (75%)\n",
      "Saved model to ../experiment/model_50.pth. You can run `python evaluate.py --model ../experiment/model_50.pth` to generate the Kaggle formatted csv file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the classifier \n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_classifier(model, train_loader, optimizer, lr_scheduler, criterion, epoch)\n",
    "    val_acc=validation_classifier(model, criterion, val_loader)\n",
    "    if val_acc>=68:\n",
    "      # Save only when it is good enough\n",
    "        model_file = experiment + '/model_' + str(epoch) + '.pth'\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "        print('Saved model to ' + model_file + '. You can run `python evaluate.py --model ' + model_file + '` to generate the Kaggle formatted csv file\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c40068f-9afd-46b8-9b61-43d5995d725e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_21686/668683560.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01187a4e-7a42-47c7-8f6d-94193184dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667d6c7-edfd-4dc9-bb6a-dfaffd6b0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Test Time Augmentation\n",
    "\n",
    "# Test features\n",
    "features_test = torch.load(os.path.join(load_dir, 'birds_features_test.pt'), map_location=torch.device(device))\n",
    "features_tensor = torch.stack([i for i in features_test])\n",
    "\n",
    "with open(\"../experiment/test_paths.txt\", \"r\") as file:\n",
    "    test_paths = file.read().split(\"\\n\")\n",
    "    \n",
    "#paths_tensor = torch.stack(test_paths)\n",
    "    \n",
    "best_model_path = \"../experiment/model_50.pth\"\n",
    "\n",
    "# Loading trained model\n",
    "state_dict = torch.load(best_model_path)\n",
    "model = Classifier(features_test[0].shape[0])\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2bdaa2-03e6-42f9-baf0-e548d86aac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    print('Using GPU')\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "output_file = \"../experiment/kaggle.csv\"\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Id,Category\\n\")\n",
    "    for path, embedding in tqdm(zip(test_paths, features_tensor)):\n",
    "        if use_cuda:\n",
    "            embedding = embedding.cuda()\n",
    "        output = model(embedding)\n",
    "        pred = output.data.max(0, keepdim=True)[1]\n",
    "        file.write(\"%s,%d\\n\" % (path, pred))\n",
    "    print(\n",
    "        \"Succesfully wrote \"\n",
    "        + output_file\n",
    "        + \", you can upload this file to the kaggle competition website\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
