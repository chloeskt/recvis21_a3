{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0374cefe-917b-499b-994e-6e070874a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/kaliayev/Documents/recvis21_a3\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "# Data initialization and loading\n",
    "from src.data import data_transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196e36ce-5547-4c56-b8b8-da1a3533e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DATA_PATH = \"../299_cropped_bird_dataset\"\n",
    "DATA_TRANSFORMS = transforms.Compose(\n",
    "    [\n",
    "        transforms.transforms.Resize((299, 299)),\n",
    "        transforms.RandomHorizontalFlip(0.3),\n",
    "            transforms.RandomRotation(degrees=(-45, 45)),\n",
    "            transforms.RandomPerspective(distortion_scale=0.4, p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                torch.nn.ModuleList(\n",
    "                    [\n",
    "                        transforms.ColorJitter(\n",
    "                            brightness=0.3, contrast=0.3, saturation=0.1, hue=0.4\n",
    "                        ),\n",
    "                        # transforms.RandomCrop(size=(32, 32)),\n",
    "                        # # transforms.RandomVerticalFlip(p=0.5),\n",
    "                        # transforms.Grayscale(num_output_channels=3),\n",
    "                    ]\n",
    "                ),\n",
    "                p=0.2,\n",
    "            ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "EMBEDDINGS_PATH = \"../New_Global_embeddings\"\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269921c-2ecb-472f-8c07-d035cd2fa2cf",
   "metadata": {},
   "source": [
    "## Extract embeddings for images using a model pretrained on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e81946c-37b6-4ab8-968d-241e96f6148c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kaliayev/.cache/torch/hub/facebookresearch_WSL-Images_main\n",
      "/home/kaliayev/Documents/recvis21_a3/env/lib/python3.8/site-packages/torchvision/models/inception.py:81: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of inception_v3 will be changed in future releases of '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start working on:  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68/68 [23:59<00:00, 21.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings folder\n",
      "Saving all features and labels for train\n",
      "Saved 1082 features map and 1082 labels\n",
      "Start working on:  val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [02:16<00:00, 19.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving all features and labels for val\n",
      "Saved 103 features map and 103 labels\n",
      "Start working on:  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [11:26<00:00, 20.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving all features and labels for test\n",
      "Saved 517 features map and 517 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.FeatureExtractorTwoHeads import FeatureExtractorTwoHeads\n",
    "\n",
    "feature_extractor = FeatureExtractorTwoHeads(\n",
    "    model_name = \"Inceptionv3\", \n",
    "    model_path = None,\n",
    "    data_path = DATA_PATH, \n",
    "    dest_path = EMBEDDINGS_PATH, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    data_transforms = DATA_TRANSFORMS, \n",
    "    device = device,\n",
    "    custom_model = False\n",
    ")\n",
    "feature_extractor.extract_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
